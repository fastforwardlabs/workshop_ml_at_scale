{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Building the Model\n",
    "\n",
    "This notebook goes through the process of selecting and building the best model to use with this problem. There are several ML algorithm options available in the SparkML library. As this is a binary classification problem, we will use Logistic Regression, Random Forest and Gradient Boosted Tree classifiers.\n",
    "\n",
    "The aim here is to train all three kinds of models and then test to see which provides the best results. The next step is to optimise i.e. the chosen models hyper parameters to then have the best option for solving this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Spark Session\n",
    "Note the changes to `spark.executor.memory` and `spark.executor.instances`. SparkML processes are more memory intensive and therefore require a different configuration to run efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Airline ML\")\\\n",
    "    .config(\"spark.executor.memory\",\"8g\")\\\n",
    "    .config(\"spark.executor.cores\",\"4\")\\\n",
    "    .config(\"spark.driver.memory\",\"6g\")\\\n",
    "    .config(\"spark.executor.instances\",\"3\")\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\",\"s3a://prod-cdptrialuser19-trycdp-com/cdp-lake/data/\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Data\n",
    "The data that is imported was the smaller data set created in Part 2. This is now stored in S3 in parquet format and will therefore load much quicker. To get the larger ML jobs to complete in a reasonable amount of time for a workshop audience, there total data set size is limited to approx 100,000 sampled rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[WEEK: double, FL_DATE: timestamp, OP_CARRIER: string, OP_CARRIER_FL_NUM: string, ORIGIN: string, DEST: string, CRS_DEP_TIME: string, CRS_ARR_TIME: string, CANCELLED: double, CRS_ELAPSED_TIME: double, DISTANCE: double]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flight_df_original =spark.read.parquet(\n",
    "#   \"s3a://prod-cdptrialuser19-trycdp-com/cdp-lake/data/airlines/airline_parquet\",\n",
    "# )\n",
    "\n",
    "flight_df_original = spark.sql(\"select * from smaller_flight_table\")\n",
    "\n",
    "flight_df = flight_df_original.na.drop().sample(1/600,seed=111)\n",
    "flight_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://spark-3h74lwxwq9dxh5v9.ml-b115a23e-db3.cdptrial.ud12-k3r3.cloudera.site\">Spark UI</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adding a link to the Spark UI for demo purposes\n",
    "from IPython.core.display import HTML\n",
    "import os\n",
    "HTML('<a href=\"http://spark-{}.{}\">Spark UI</a>'.format(os.getenv(\"CDSW_ENGINE_ID\"),os.getenv(\"CDSW_DOMAIN\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- WEEK: double (nullable = true)\n",
      " |-- FL_DATE: timestamp (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: string (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: string (nullable = true)\n",
      " |-- CRS_ARR_TIME: string (nullable = true)\n",
      " |-- CANCELLED: double (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
      " |-- DISTANCE: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Feature Engineering\n",
    "Before applying any training to an ML algorithm, you need to check if the format of columns in your dataset will work with the chosen algorithm. The first change that we need to make is extracting the hour of the day from the time. This is based on an assumption of seasonality during the day. The second change is to extract week of the year using `weekofyear`. The ML aglorithms needs floating point numbers for optimal internal matrix maths, therefore these two new columns are cast using `cast('double')`.\n",
    "\n",
    "> HANDY TIP\n",
    "> \n",
    "> Each new release of PySpark brings with it more useful functions that makes it easier to do things that you had to use a UDF to do.\n",
    "> Even with Apache Arrow used for memory storage, using a Python UDF in a pyspark tranform is going to be slower than running pure Spark functions. After spending some time reading through the pyspark function list, I found that I could do what my udf did using a the `when` and `otherwise` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf,substring,weekofyear,concat,col,when,length,lit,hour\n",
    "\n",
    "#No longer needed\n",
    "#convert_time_to_hour = udf(lambda x: x if len(x) == 4 else \"0{}\".format(x),StringType())\n",
    "\n",
    "flight_df = flight_df\\\n",
    "    .withColumn(\n",
    "        'CRS_DEP_HOUR',\n",
    "        when(\n",
    "            length(col(\"CRS_DEP_TIME\")) == 4,col(\"CRS_DEP_TIME\")\n",
    "        )\\\n",
    "        .otherwise(concat(lit(\"0\"),col(\"CRS_DEP_TIME\")))\n",
    "    )\n",
    "\n",
    "flight_df = flight_df.withColumn('CRS_DEP_HOUR',col('CRS_DEP_TIME').cast('double'))\n",
    "flight_df = flight_df.withColumn('WEEK',weekofyear('FL_DATE').cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- WEEK: double (nullable = true)\n",
      " |-- FL_DATE: timestamp (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: string (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: string (nullable = true)\n",
      " |-- CRS_ARR_TIME: string (nullable = true)\n",
      " |-- CANCELLED: double (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
      " |-- DISTANCE: double (nullable = true)\n",
      " |-- CRS_DEP_HOUR: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Feature Indexing and Encoding\n",
    "\n",
    "The next feature engineering requirement is to convert the categorical variables into a numeric form. The `OP_CARRIER`,`ORIGIN` and `DEST` columns are the ones that will be used. All three are `string` type categorical variables. There are 2 methods of dealing with these, one is using a [String Indexer](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.StringIndexer) and the other is using a [One Hot Encoder](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.OneHotEncoder). Logsitic Regression requires one hot encoding, [tree based algorithims do not](https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/). The code below creates a [Spark Pipeline](https://spark.apache.org/docs/latest/ml-pipeline.html) that assembles the process to get the columns needed, do the indexing and encoding and create an packaged vector that the various models will be trained on.\n",
    "\n",
    "> HANDY TIP\n",
    ">\n",
    "> `OneHotEncoderEstimator` is a function introduced in Spark 2.3 and works on severnal indexed columns in one go, rather than having to do one `OneHotEncoder` per indexed feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, OneHotEncoderEstimator\n",
    "\n",
    "numeric_cols = [\"CRS_ELAPSED_TIME\",\"DISTANCE\",\"WEEK\",\"CRS_DEP_HOUR\"]\n",
    "\n",
    "op_carrier_indexer = StringIndexer(inputCol ='OP_CARRIER', outputCol = 'OP_CARRIER_INDEXED',handleInvalid=\"keep\")\n",
    "\n",
    "origin_indexer = StringIndexer(inputCol ='ORIGIN', outputCol = 'ORIGIN_INDEXED',handleInvalid=\"keep\")\n",
    "\n",
    "dest_indexer = StringIndexer(inputCol ='DEST', outputCol = 'DEST_INDEXED',handleInvalid=\"keep\")\n",
    "\n",
    "indexer_encoder = OneHotEncoderEstimator(\n",
    "    inputCols = ['OP_CARRIER_INDEXED','ORIGIN_INDEXED','DEST_INDEXED'],\n",
    "    outputCols= ['OP_CARRIER_ENCODED','ORIGIN_ENCODED','DEST_ENCODED']\n",
    ")\n",
    "\n",
    "input_cols=[\n",
    "    'OP_CARRIER_ENCODED',\n",
    "    'ORIGIN_ENCODED',\n",
    "    'DEST_ENCODED'] + numeric_cols\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = input_cols,\n",
    "    outputCol = 'features')\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages=[\n",
    "        op_carrier_indexer,\n",
    "        origin_indexer,\n",
    "        dest_indexer,\n",
    "        indexer_encoder,\n",
    "        assembler]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Training and Test Datasets\n",
    "Once the pipeline has been built the next step is to `fit` the pipeline to the data, and then `transform` it into the final vector form. There after the data needs to split into a test and training set. This is done using `randomSplit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CANCELLED: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipelineModel = pipeline.fit(flight_df)\n",
    "model_df = pipelineModel.transform(flight_df)\n",
    "selectedCols = ['CANCELLED', 'features']# + cols\n",
    "model_df = model_df.select(selectedCols)\n",
    "model_df.printSchema()\n",
    "(train, test) = model_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - Logistic Regression\n",
    "Now that the vector is assembled and ready to run, the first model we will try is good old faithful logsitic regression.\n",
    "\n",
    "_Note: this is a bit too accruate to be considered valid_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'CANCELLED', maxIter=10)\n",
    "\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6487914193492051"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "predictionslr = lrModel.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"CANCELLED\",metricName=\"areaUnderROC\")\n",
    "evaluator.evaluate(predictionslr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Pipeline model\n",
    "This next cell writes the fitted pipeline model back to S3 so it can be used for other processes to build/trainin different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipelineModel.write().overwrite().save(\"s3a://ml-field/demo/flight-analysis/data/models/pipeline_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebuild the Pipeline\n",
    "For the tree based classifiers, the data does not need to be one hot encoded. Being able to use the string indexed values significantly reduces the dimensionality of the feature vecotor and vastly improves processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CANCELLED: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_cols = [\n",
    "    'OP_CARRIER_INDEXED',\n",
    "    'ORIGIN_INDEXED',\n",
    "    'DEST_INDEXED'] + numeric_cols\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = input_cols,\n",
    "    outputCol = 'features')\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages=[\n",
    "        op_carrier_indexer,\n",
    "        origin_indexer,\n",
    "        dest_indexer,\n",
    "        assembler]\n",
    ")\n",
    "\n",
    "pipelineModel = pipeline.fit(flight_df)\n",
    "\n",
    "model_df = pipelineModel.transform(flight_df)\n",
    "selectedCols = ['CANCELLED', 'features']\n",
    "model_df = model_df.select(selectedCols)\n",
    "model_df.printSchema()\n",
    "(train, test) = model_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - Random Forest\n",
    "The `RandomForestClassifier` is run on the updated pipeline and the results are really bad. Its bad performance is likely because of the large number of categories in the categorical variables. To get it to even run, the `maxBins` setting had to be increased to 390, which far above the default of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rfclassifier = RandomForestClassifier(labelCol = 'CANCELLED', featuresCol = 'features', maxBins=390)\n",
    "rfmodel = rfclassifier.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6298042064110775"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "predictionsrf = rfmodel.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"CANCELLED\",metricName=\"areaUnderROC\")\n",
    "evaluator.evaluate(predictionsrf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - Gradient Boosted Tree\n",
    "The GBTClassifier ran for the longest of all three algorithms but did perform significantly better than the the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(maxIter=10,featuresCol = 'features', labelCol = 'CANCELLED', maxBins=390)\n",
    "\n",
    "gbtModel = gbt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6286831124808314"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionsgbt = gbtModel.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"CANCELLED\",metricName=\"areaUnderROC\")\n",
    "evaluator.evaluate(predictionsgbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising The Model\n",
    "The final step is to pick the model that peformed the best and then try optimising the various hyper parameters to see if the model's performance can be improved. For linear regression the three parameters that can have a material difference are:\n",
    "\n",
    "+ `regParam`\n",
    "+ `elasticNetParam`\n",
    "+ `maxIter`\n",
    "\n",
    "To get a list of all the parameters in a model, run `print(<your model object>.explainParams())`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial. (default: auto)\n",
      "featuresCol: features column name (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term (default: True)\n",
      "labelCol: label column name (default: label, current: CANCELLED)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "maxIter: maximum number of iterations (>= 0) (default: 100, current: 10)\n",
      "predictionCol: prediction column name (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0) (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model (default: True)\n",
      "threshold: threshold in binary classification prediction, in range [0, 1] (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0) (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(lrModel.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lrModel.maxIter, [5, 10,100])\n",
    "             .addGrid(lrModel.regParam, [0.0, 0.01, 0.1])\n",
    "             .addGrid(lrModel.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .build())\n",
    "\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=2, parallelism=3)\n",
    "\n",
    "cvModel = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6422277836738305"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = cvModel.transform(test)\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regParam = 0.0\n",
      "elasticNetParam = 0.5\n",
      "maxIter = 100\n"
     ]
    }
   ],
   "source": [
    "print(\"regParam = {}\".format(cvModel.bestModel._java_obj.getRegParam()))\n",
    "print(\"elasticNetParam = {}\".format(cvModel.bestModel._java_obj.getElasticNetParam()))\n",
    "print(\"maxIter = {}\".format(cvModel.bestModel._java_obj.getMaxIter()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "#trainingSummary = cvModel.bestModel.summary\n",
    "trainingSummary = lrModel.summary\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "def plotter():\n",
    "    plt.plot(roc['FPR'],roc['TPR'])\n",
    "    plt.ylabel('False Positive Rate')\n",
    "    plt.xlabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "plotter()\n",
    "#print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
